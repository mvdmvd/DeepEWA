{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"/Users/meesvandartel/Desktop/Coursework/CGT/DeepEWA/EWA_imp.jl\")\n",
    "using .EWA, Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Attraction updating function:\n",
    "$$\n",
    "Q_{i}^{\\mu}(t) = \\frac{(1-\\alpha) N(t-1) Q_{i}^{\\mu}(t-1)}{N(t)} + \\frac{\\left[ \\delta + (1-\\delta) \\mathbb{I}(s_i^\\mu,s^{-\\mu}(t)) \\right] \\Pi^\\mu(s_i^\\mu, s^{-\\mu}(t))}{N(t)}\n",
    "$$\n",
    "\n",
    "#### Mixed strategy determination:\n",
    "$$\n",
    "\\sigma^{\\mu}(t)=\\frac{e^{\\beta Q_1^R (t)}}{e^{\\beta Q_1^R (t)} + e^{\\beta Q_2^R (t)}}\n",
    "$$\n",
    "\n",
    "\n",
    "#### Special Cases:\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "&\\text{best response dynamics:  }\\alpha=1, \\quad \\beta = +\\infty, \\quad \\delta = 1, \\quad \\forall \\kappa \\in [0,1] \\\\\n",
    "&\\text{reinforcement learning:  }\\delta = 0, \\quad \\forall \\kappa \\in [0,1] \\\\\n",
    "&\\quad\\text{average RL: } \\quad \\kappa=0, \\quad \\text{cummulative RL: } \\quad \\kappa = 1 \\\\\n",
    "&\\text{ficticious play:  }\\alpha=0, \\quad \\beta = +\\infty, \\quad \\delta = 1, \\quad  \\kappa = 0\\\\\n",
    "&\\quad\\text{stochastic ficticious play:  } \\beta < +\\infty\\\\\n",
    "&\\text{replicator dynamics:  } \\beta \\rightarrow 0, \\quad \\alpha = 0, \\quad \\delta = 1, \\quad \\forall \\kappa \\in (0,1]\\\\\n",
    "&\\text{logit dynamics:  } \\alpha = 1, \\quad \\delta = 1,\\quad \\kappa = 1\n",
    "\\end{aligned}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior Q: [[0.0, 0.0], [0.0, 0.0]], prior N: 0.0\n",
      "Iter 1: σ=[[0.5, 0.5], [0.5, 0.5]], s=[2, 2], Q=[[1.0, -1.0], [-1.0, 1.0]], N=1.0\n",
      "Iter 2: σ=[[1.0, 0.0], [0.0, 1.0]], s=[1, 2], Q=[[1.0, -1.0], [1.0, -1.0]], N=1.0\n",
      "Iter 3: σ=[[1.0, 0.0], [1.0, 0.0]], s=[1, 1], Q=[[-1.0, 1.0], [1.0, -1.0]], N=1.0\n",
      "Iter 4: σ=[[0.0, 1.0], [1.0, 0.0]], s=[2, 1], Q=[[-1.0, 1.0], [-1.0, 1.0]], N=1.0\n",
      "Iter 5: σ=[[0.0, 1.0], [0.0, 1.0]], s=[2, 2], Q=[[1.0, -1.0], [-1.0, 1.0]], N=1.0\n",
      "Iter 6: σ=[[1.0, 0.0], [0.0, 1.0]], s=[1, 2], Q=[[1.0, -1.0], [1.0, -1.0]], N=1.0\n",
      "Iter 7: σ=[[1.0, 0.0], [1.0, 0.0]], s=[1, 1], Q=[[-1.0, 1.0], [1.0, -1.0]], N=1.0\n",
      "Iter 8: σ=[[0.0, 1.0], [1.0, 0.0]], s=[2, 1], Q=[[-1.0, 1.0], [-1.0, 1.0]], N=1.0\n",
      "Iter 9: σ=[[0.0, 1.0], [0.0, 1.0]], s=[2, 2], Q=[[1.0, -1.0], [-1.0, 1.0]], N=1.0\n",
      "Iter 10: σ=[[1.0, 0.0], [0.0, 1.0]], s=[1, 2], Q=[[1.0, -1.0], [1.0, -1.0]], N=1.0\n",
      "Iter 500: σ=[[0.0, 1.0], [1.0, 0.0]], s=[2, 1], Q=[[-1.0, 1.0], [-1.0, 1.0]], N=1.0\n",
      "Iter 1000: σ=[[0.0, 1.0], [1.0, 0.0]], s=[2, 1], Q=[[-1.0, 1.0], [-1.0, 1.0]], N=1.0\n",
      "Iter 5000: σ=[[0.0, 1.0], [1.0, 0.0]], s=[2, 1], Q=[[-1.0, 1.0], [-1.0, 1.0]], N=1.0\n",
      "Converged at iter: 5051\n",
      "____________________________________________________________________________________________________________________________\n",
      "Q*: [[-1.0, 1.0], [-1.0, 1.0]], N*: 1.0\n",
      "Game: [[-1 1; 1 -1], [1 -1; -1 1]]\n",
      "Converged mixed strategy vector: σ =  [[0.0, 1.0], [1.0, 0.0]]\n",
      "Frequencies of player 1: s₁ = 0.5001980198019802, s₂ = 0.4998019801980198\n",
      "Frequencies of player 2: s₁ = 0.5001980198019802, s₂ = 0.4998019801980198\n"
     ]
    }
   ],
   "source": [
    "# payoff matrixes\n",
    "coord = [[5 1; 1 4], [5 1; 1 4]]\n",
    "pris = [[5 0; 20 1], [5 0; 20 1]]\n",
    "pennies = [[-1 1; 1 -1], [1 -1; -1 1]]\n",
    "cyclic = [[5 1;1 4],[-5 1; 1 -4]]\n",
    "mixed = [[9 2;3 6], [1 7; 8 4]] # The unique mixed NE is [[0.3, 0.7], [0.4, 0.6]] (analytically)\n",
    "mixed_2 = [[2 1;1 4],[-3 1; 2 -1]] # mixed NE is 2/7, 5/7; 3/4, 1/4 analytically\n",
    "\n",
    "payoff = pennies # game\n",
    "params = EWA.init_EWA(\n",
    "Q₀=[[0.0, 0.0], [0.0, 0.0]], N₀=0.0, # priors\n",
    "α=0.0, δ=1.0, κ=0.0, β=Inf64, # params\n",
    "\n",
    "payoff=payoff)       \n",
    "Qₜ, Nₜ, sₜ, σₜ, freq_1, freq_2 = EWA.run_EWA(params)\n",
    "\n",
    "println(\"____________________________________________________________________________________________________________________________\")\n",
    "println(\"Q*: $Qₜ, N*: $Nₜ\")\n",
    "println(\"Game: $payoff\")\n",
    "println(\"Converged mixed strategy vector: σ =  $σₜ\")\n",
    "\n",
    "p₂ = 1-freq_1\n",
    "println(\"Frequencies of player 1: s₁ = $freq_1, s₂ = $p₂\")\n",
    "\n",
    "p₄ = 1-freq_2\n",
    "println(\"Frequencies of player 2: s₁ = $freq_2, s₂ = $p₄\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.4",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
