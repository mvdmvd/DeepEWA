{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: replacing module EWA.\n"
     ]
    }
   ],
   "source": [
    "include(\"/Users/meesvandartel/Desktop/Coursework/CGT/DeepEWA/EWA_imp.jl\")\n",
    "using .EWA, Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Attraction updating function:\n",
    "$$\n",
    "Q_{i}^{\\mu}(t) = \\frac{(1-\\alpha) N(t-1) Q_{i}^{\\mu}(t-1)}{N(t)} + \\frac{\\left[ \\delta + (1-\\delta) \\mathbb{I}(s_i^\\mu,s^{-\\mu}(t)) \\right] \\Pi^\\mu(s_i^\\mu, s^{-\\mu}(t))}{N(t)}\n",
    "$$\n",
    "\n",
    "#### Mixed strategy determination:\n",
    "$$\n",
    "\\sigma^{\\mu}(t)=\\frac{e^{\\beta Q_1^R (t)}}{e^{\\beta Q_1^R (t)} + e^{\\beta Q_2^R (t)}}\n",
    "$$\n",
    "\n",
    "\n",
    "#### Special Cases:\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "&\\text{best response dynamics:  }\\alpha=1, \\quad \\beta = +\\infty, \\quad \\delta = 1, \\quad \\forall \\kappa \\in [0,1] \\\\\n",
    "&\\text{reinforcement learning:  }\\delta = 0, \\quad \\forall \\kappa \\in [0,1] \\\\\n",
    "&\\quad\\text{average RL: } \\quad \\kappa=0, \\quad \\text{cummulative RL: } \\quad \\kappa = 1 \\\\\n",
    "&\\text{ficticious play:  }\\alpha=0, \\quad \\beta = +\\infty, \\quad \\delta = 1, \\quad  \\kappa = 0\\\\\n",
    "&\\quad\\text{stochastic ficticious play:  } \\beta < +\\infty\\\\\n",
    "&\\text{replicator dynamics:  } \\beta \\rightarrow 0, \\quad \\alpha = 0, \\quad \\delta = 1, \\quad \\forall \\kappa \\in (0,1]\\\\\n",
    "&\\text{logit dynamics:  } \\alpha = 1, \\quad \\delta = 1,\\quad \\kappa = 1\n",
    "\\end{aligned}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior Q: [[0.0, 0.0], [0.0, 0.0]], prior N: 0.0\n",
      "Iter 1: σ=[[0.5, 0.5], [0.5, 0.5]], s=[1, 2], Q=[[1.0, 4.0], [-3.0, 2.0]], N=1.0\n",
      "Iter 2: σ=[[0.0, 1.0], [0.0, 1.0]], s=[2, 2], Q=[[1.0, 4.0], [-1.0, 0.5]], N=2.0\n",
      "Iter 3: σ=[[0.0, 1.0], [0.0, 1.0]], s=[2, 2], Q=[[1.0, 4.0], [-0.3333333333333333, 0.0]], N=3.0\n",
      "Iter 4: σ=[[0.0, 1.0], [0.0, 1.0]], s=[2, 2], Q=[[1.0, 4.0], [0.0, -0.25]], N=4.0\n",
      "Iter 5: σ=[[0.0, 1.0], [1.0, 0.0]], s=[2, 1], Q=[[1.2, 3.4], [0.2, -0.4]], N=5.0\n",
      "Iter 6: σ=[[0.0, 1.0], [1.0, 0.0]], s=[2, 1], Q=[[1.3333333333333333, 3.0], [0.3333333333333333, -0.5]], N=6.0\n",
      "Iter 7: σ=[[0.0, 1.0], [1.0, 0.0]], s=[2, 1], Q=[[1.4285714285714286, 2.7142857142857144], [0.42857142857142855, -0.5714285714285714]], N=7.0\n",
      "Iter 8: σ=[[0.0, 1.0], [1.0, 0.0]], s=[2, 1], Q=[[1.5, 2.5], [0.5, -0.625]], N=8.0\n",
      "Iter 9: σ=[[0.0, 1.0], [1.0, 0.0]], s=[2, 1], Q=[[1.5555555555555556, 2.3333333333333335], [0.5555555555555556, -0.6666666666666666]], N=9.0\n",
      "Iter 10: σ=[[0.0, 1.0], [1.0, 0.0]], s=[2, 1], Q=[[1.6, 2.2], [0.6, -0.7]], N=10.0\n",
      "Iter 500: σ=[[0.0, 1.0], [1.0, 0.0]], s=[2, 1], Q=[[1.7159999999999997, 1.8519999999999999], [-0.08000000000000006, -0.18999999999999972]], N=500.0\n",
      "Iter 1000: σ=[[0.0, 1.0], [1.0, 0.0]], s=[2, 1], Q=[[1.7499999999999996, 1.75], [-8.526512829121202e-17, -0.24999999999999983]], N=1000.0\n",
      "Iter 5000: σ=[[1.0, 0.0], [0.0, 1.0]], s=[1, 2], Q=[[1.7511999999999979, 1.7463999999999993], [-0.15920000000000026, -0.1306000000000007]], N=5000.0\n",
      "Iter 10000: σ=[[0.0, 1.0], [1.0, 0.0]], s=[2, 1], Q=[[1.7410999999999992, 1.7766999999999977], [-0.13960000000000014, -0.14529999999999993]], N=10000.0\n",
      "Converged at iter: 14935\n",
      "____________________________________________________________________________________________________________________________\n",
      "Q*: [[1.7457820032137112, 1.762653990358863], [-0.12587038028923467, -0.1555972147830745]], N*: 14936.0\n",
      "Game: [[2 1; 1 4], [-3 1; 2 -1]]\n",
      "Converged mixed strategy vector: σ =  [[0.0, 1.0], [1.0, 0.0]]\n",
      "Frequencies of player 1: s₁ = 0.28143832864604257, s₂ = 0.7185616713539574\n",
      "Frequencies of player 2: s₁ = 0.7458818802732021, s₂ = 0.2541181197267979\n"
     ]
    }
   ],
   "source": [
    "# payoff matrixes\n",
    "coord = [[5 1; 1 4], [5 1; 1 4]]\n",
    "pris = [[5 0; 20 1], [5 0; 20 1]]\n",
    "pennies = [[-1 1; 1 -1], [1 -1; -1 1]]\n",
    "mixed = [[9 2;3 6], [1 7; 8 4]] # The unique mixed NE is [[0.3, 0.7], [0.4, 0.6]] (analytically)\n",
    "mixed_2 = [[2 1;1 4],[-3 1; 2 -1]]\n",
    "\n",
    "payoff = mixed_2 # game\n",
    "params = EWA.init_EWA(\n",
    "Q₀=[[0.0, 0.0], [0.0, 0.0]], N₀=0.0, # priors\n",
    "α=0.0, δ=1.0, κ=0.0, β=Inf64, # params\n",
    "\n",
    "payoff=payoff)       \n",
    "Qₜ, Nₜ, sₜ, σₜ, freq_1, freq_2 = EWA.run_EWA(params)\n",
    "\n",
    "println(\"____________________________________________________________________________________________________________________________\")\n",
    "println(\"Q*: $Qₜ, N*: $Nₜ\")\n",
    "println(\"Game: $payoff\")\n",
    "println(\"Converged mixed strategy vector: σ =  $σₜ\")\n",
    "\n",
    "p₂ = 1-freq_1\n",
    "println(\"Frequencies of player 1: s₁ = $freq_1, s₂ = $p₂\")\n",
    "\n",
    "p₄ = 1-freq_2\n",
    "println(\"Frequencies of player 2: s₁ = $freq_2, s₂ = $p₄\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.4",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
