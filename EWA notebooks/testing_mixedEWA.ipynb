{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: replacing module EWA.\n"
     ]
    }
   ],
   "source": [
    "include(\"/Users/meesvandartel/Desktop/Coursework/CGT/DeepEWA/DeepEWA_funs&structs.jl\")\n",
    "using .EWA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Attraction updating function:\n",
    "$$\n",
    "Q_{i}^{\\mu}(t) = \\frac{(1-\\alpha) N(t-1) Q_{i}^{\\mu}(t-1)}{N(t)} + \\frac{\\left[ \\delta + (1-\\delta) \\mathbb{I}(s_i^\\mu,s^{-\\mu}(t)) \\right] \\Pi^\\mu(s_i^\\mu, s^{-\\mu}(t))}{N(t)}\n",
    "$$\n",
    "\n",
    "#### Mixed strategy determination:\n",
    "$$\n",
    "\\sigma^{\\mu}(t)=\\frac{e^{\\beta Q_1^R (t)}}{e^{\\beta Q_1^R (t)} + e^{\\beta Q_2^R (t)}}\n",
    "$$\n",
    "\n",
    "\n",
    "#### Special Cases:\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "&\\text{best response dynamics:  }\\alpha=1, \\quad \\beta = +\\infty, \\quad \\delta = 1, \\quad \\forall \\kappa \\in [0,1] \\\\\n",
    "&\\text{reinforcement learning:  }\\delta = 0, \\quad \\forall \\kappa \\in [0,1] \\\\\n",
    "&\\quad\\text{average RL: } \\quad \\kappa=0, \\quad \\text{cummulative RL: } \\quad \\kappa = 1 \\\\\n",
    "&\\text{ficticious play:  }\\alpha=0, \\quad \\beta = +\\infty, \\quad \\delta = 1, \\quad  \\kappa = 0\\\\\n",
    "&\\quad\\text{stochastic ficticious play:  } \\beta < +\\infty\\\\\n",
    "&\\text{replicator dynamics:  } \\beta \\rightarrow 0, \\quad \\alpha = 0, \\quad \\delta = 1, \\quad \\forall \\kappa \\in (0,1]\\\\\n",
    "&\\text{logit dynamics:  } \\alpha = 1, \\quad \\delta = 1,\\quad \\kappa = 1\n",
    "\\end{aligned}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________________________________\n",
      "Q*: [[5.0, 1.0], [5.0, 1.0]], N*: 103.0\n",
      "Game: [[5 1; 1 4], [5 1; 1 4]]\n",
      "Converged mixed strategy vector: σ =  [[1.0, 0.0], [1.0, 0.0]]\n",
      "Converged solution: [[1.00990099009901, -0.00990099009900991], [1.00990099009901, -0.00990099009900991]]\n",
      "NE found!\n",
      "True Nash Equilibria: [[[1.0, 0.0], [1.0, 0.0]], [[0.0, 1.0], [0.0, 1.0]], [[0.4285714285714285, 0.5714285714285714], [0.4285714285714285, 0.5714285714285714]]] \n"
     ]
    }
   ],
   "source": [
    "# payoff matrixes\n",
    "coord = [[5 1; 1 4], [5 1; 1 4]]\n",
    "pris = [[5 0; 20 1], [5 0; 20 1]]\n",
    "pennies = [[-1 1; 1 -1], [1 -1; -1 1]]\n",
    "cyclic = [[5 1;1 4],[-5 1; 1 -4]]\n",
    "mixed = [[9 2;3 6], [1 7; 8 4]] # The unique mixed NE is [[0.3, 0.7], [0.4, 0.6]] (analytically)\n",
    "mixed_2 = [[2 1;1 4],[-3 1; 2 -1]] # mixed NE is 2/7, 5/7; 3/4, 1/4 analytically\n",
    "\n",
    "payoff = coord # game\n",
    "params = EWA.init_EWA(\n",
    "Q₀=[[0.0, 0.0], [0.0, 0.0]], N₀=0.0, # priors\n",
    "α=0.0, δ=1.0, κ=0.0, β=Inf64, # params\n",
    "\n",
    "payoff=payoff)       \n",
    "Qₜ, Nₜ, sₜ, σₜ, conv_sol, NE_found, NE = EWA.run_EWA_mixed(params)\n",
    "\n",
    "println(\"____________________________________________________________________________________________________________________________\")\n",
    "println(\"Q*: $Qₜ, N*: $Nₜ\")\n",
    "println(\"Game: $payoff\")\n",
    "println(\"Converged mixed strategy vector: σ =  $σₜ\")\n",
    "println(\"Converged solution: $conv_sol\")\n",
    "if NE_found == true\n",
    "    println(\"NE found!\")\n",
    "end\n",
    "\n",
    "println(\"True Nash Equilibria: $NE \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.4",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
